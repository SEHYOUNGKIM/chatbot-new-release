import streamlit as st
# pypdf tiktoken  faiss-cpu
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
#from langchain.vectorstores import Chroma
from langchain.document_loaders import TextLoader
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

if "messages" not in st.session_state:
    st.session_state["messages"] = ""


st.header("ì‹ í•œAI, íˆ¬ì ë³´ê³ ì„œ ê¸°ë°˜ ì±—ë´‡")
st.subheader("made by TopGunğŸ›©ï¸")
st.text('ë°˜ë“œì‹œ api í‚¤ë¥¼ ì…ë ¥í•˜ê³  ì—”í„°ë¥¼ ë¨¼ì € ëˆŒëŸ¬ì£¼ì„¸ìš”.')



option = st.selectbox(
'ë³´ê³ ì„œë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.',
('ìŒì‹ë£Œ.pdf', 'í™”ì¥í’ˆ ë° ì„¬ìœ .pdf', 'ì‹ í•œ-ì—°êµ¬ë³´ê³ ì„œ-1.pdf', 'ì‹ í•œ-ì—°êµ¬ë³´ê³ ì„œ-2.pdf'))



API_KEY = st.sidebar.text_input(":blue[Enter Your OPENAI API-KEY :]", 
                    placeholder="ë³¸ì¸ì˜ api í‚¤ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”! (sk-...)",
                    type="password")

if API_KEY != "":

    loader = PyPDFLoader(option)
    data = loader.load()

    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0, separator = "\n",)
    documents = text_splitter.split_documents(data)

    embeddings = OpenAIEmbeddings(openai_api_key = API_KEY)
    chat_model = OpenAI(temperature=0, openai_api_key = API_KEY)
    db = FAISS.from_documents(documents, embeddings)



    template = """
Given the following extracted parts of a long document and a question, create a final answer, less than 100 words.
document_context: {context}
chat_history: {chat_history}
human_input: {human_input}
"""

    usr_input = st.text_input("ì‹ í•œ-ë¦¬í¬íŠ¸-ì±—ë´‡", placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    prompt_result = st.empty()
    prompt_result.text_area("ì‹ í•œ-ë¦¬í¬íŠ¸-ì±—ë´‡", height=400)

    prompt = PromptTemplate(
        input_variables=["chat_history", "human_input", "context"], 
        template=template
    )

    memory = ConversationBufferMemory(memory_key="chat_history", input_key="human_input")
    chain = load_qa_chain(chat_model, chain_type="stuff", memory=memory, prompt=prompt)

# ì´ë¶€ë¶„ì„ ì´ˆê¸°í™”í•˜ë©´ ë ë“¯.
if st.button("Send"):
    with st.spinner("Generating response..."):
        
        docs = db.similarity_search(usr_input)
        result = chain({"input_documents": docs, "human_input": usr_input}, return_only_outputs=True)
        st.session_state["messages"] += "ì‹ í•œAI: " + result['output_text'] + '\n'
        prompt_result.text_area('ì‹ í•œ-ë¦¬í¬íŠ¸-ì±—ë´‡', value =  st.session_state["messages"])
        



if st.button("ì´ˆê¸°í™”"):
    memory = ""
    chain = ""
    prompt_result.text_area('ì‹ í•œ-ë¦¬í¬íŠ¸-ì±—ë´‡', value = "")